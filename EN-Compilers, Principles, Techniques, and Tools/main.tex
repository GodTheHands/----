\documentclass[a4paper,twoside]{book}
\usepackage{amd}
\usepackage{fancyvrb}
\usepackage{adjustbox}

% %--------------------------------------------------------------------------
% %         General Setting
% %--------------------------------------------------------------------------

\graphicspath{{Images/}{../Images/}} %Path of figures
\setkeys{Gin}{width=0.85\textwidth} %Size of figures
\setlength{\cftbeforechapskip}{3pt} %space between items in toc
\setlength{\parindent}{0.5cm} % Idk
\input{theorems.tex} % Theorems styles and colors
\usepackage[english]{babel} %Language
\usepackage{framed}

\setlist[itemize]{itemsep=5pt} % Adjust the length as needed
\setlist[enumerate]{itemsep=5pt} % Adjust the length as needed

% \usepackage{lmodern} %  Latin Modern font
% \usepackage{newtxtext,newtxmath}

% %--------------------------------------------------------------------------
% %         General Informations
% %--------------------------------------------------------------------------
\newcommand{\BigTitle}{
    Compilers: Principles, Techniques, \& Tools
    }

\newcommand{\LittleTitle}{
    By Alfred V. Aho et all
    }

    
\begin{document}

% %--------------------------------------------------------------------------
% %         First pages 
% %--------------------------------------------------------------------------
\newgeometry{top=8cm,bottom=.5in,left=2cm,right=2cm}
\subfile{files/0.0.0.titlepage}
\restoregeometry
\thispagestyle{empty}
\setcounter{page}{0}
\tableofcontents
\thispagestyle{empty}
\setcounter{page}{0}

% %--------------------------------------------------------------------------
% %         Core of the document 
% %--------------------------------------------------------------------------
\chapter{Introduction}

The world as we know it depends on programming languages, because all the software running on all the computers was written in some programming language. But, before a program can be run, it first must be translated into a form in which it can be executed by a computer.

The software systems that do this translation are called \textit{compilers}.

\section{Language Processors}

Simply stated, a compiler is a program that can read a program in one language--the \textit{source} language--and translate it into an equivalent program in another language--the \textit{target} language.

An \textit{interpreter} is another common kind of language processor.

The task of collecting the source program is sometimes entrusted to a separate program, called a \textit{preprocessor}.

The compiler may produce an assembly-language program as its output, because assembly language is easier to produce as output and is easier to debug. The assembly language is then processed by a program called an \textit{assembler} that produces relocatable machine code as its output.

The \textit{linker} resolves external memory addresses, where the code in one file may refer to a location in another file. The \textit{loader} then puts together all of the executable object files into memory for execution.

\section{The Structure of a Compiler}

Up to this point we have treated a compiler as a single box that maps a source program into a semantically equivalent target program. If we open up this box a little, we see that there are two parts to this mapping: analysis and synthesis.

The \textit{analysis} part breaks up the source program into constituent pieces and imposes a grammatical structure on them. The analysis part also collects information about the source program and stores it in a data structure called a \textit{symbol table}, which is passed along with the intermediate representation to the synthesis part.

The \textit{synthesis} part constructs the desired target program from the intermediate representation and the information in the symbol table. The analysis part is often called the \textit{front end} of the compiler; the synthesis part is the \textit{back end}.

If we examine the compilation process in more detail, we see that it operates as a sequence of \textit{phases}, each of which transforms one representation of the source program to another.

\subsection{Lexical Analysis}

The first phase of a compiler is called \textit{lexical analysis} or \textit{scanning}. The lexical analyzer reads the stream of characters making up the source program and groups the characters into meaningful sequences called \textit{lexemes}. For each lexeme, the lexical analyzer produces as output a \textit{token} of the form $$\langle\textit{token-name, attribute-value}\rangle$$ that is passes on to the subsequent phase, syntax analysis. In the token, the first component \textit{token-name} is an abstract symbol that is used during syntax analysis, and the second component \textit{attribute-value} points to an entry in the symbol table for this token.

\subsection{Syntax Analysis}

The second phase of the compiler is \textit{syntax analysis} or \textit{parsing}. A typical representation is a \textit{syntax tree} in which each interior node represents an operation and the children of the node represent the arguments of the operation.

\subsection{Semantic Analysis}

The \textit{semantic analyzer} uses the syntax tree and the information in the symbol table to check the source program for semantic consistency with the language definition.

An important part of semantic analysis is \textit{type checking}, where the compiler checks that each operator has matching operands.

The language specification may permit some type conversions called \textit{coercions}.

\subsection{Intermediate Code Generation}

We consider an intermediate form called \textit{three-address code}, which consists of a sequence of assembly-like instructions with three operands per instruction.

\subsection{The Grouping of Phases into Passes}

In an implementation, activities from several phases may be grouped together into a \textit{pass} that reads an input file and writes an output file.

\subsection{Compiler-Construction Tools}

Some commonly used compiler-construction tools include
\begin{enumerate}
    \item \textit{Parser generators} that automatically produce syntax analyzers from a grammatical description of a programming language.
    \item \textit{Scanner generators} that produce lexical analyzers from a regular-expression description of the tokens of a language.
    \item \textit{Syntax-directed translation engines} that produce collections of routines for walking a parse tree and generating intermediate code.
    \item \textit{Code-generator generators} that produce a code generator from a collection of rules for translating each operation of the intermediate language into the machine language for a target machine.
    \item \textit{Data-flow analysis engines} that facilitate the gathering of information about how values are transmitted from one part of a program to each other part.
    \item \textit{Compiler-construction toolkits} that provide an integrated set of routines for construction various phases of a compiler.
\end{enumerate}

\section{The Evolution of Programming Language}
\subsection{The Move to Higher-Level Languages}

One classification is by generation. \textit{First-generation languages} are the machine languages, \textit{second-generation} the assembly languages, and \textit{third-generation} the higher-level languages. \textit{Fourth-generation languages} are languages designed for specific applications. The term \textit{fifth-generation language} has been applied to logic- and constraint-based languages.

Another classification of languages uses the term \textit{imperative} for languages in which a program specifies \textit{how} a computation is to be done and \textit{declarative} for languages in which a program specifies \textit{what} computation is to be done.

The term \textit{von Neumann language} is applied to programming languages whose computational model is based on the von Neumann computer architecture.

An \textit{object-oriented language} is one that supports object-oriented programming, a programming style in which a program consists of a collection of objects that interact with one another.

\textit{Scripting languages} are interpreted languages with high-level operators designed for "gluing together" computations.

\section{Applications of Compiler Technology}
\subsection{implementation of High-Level Programming Languages}

A body of compiler optimizations, known as \textit{data-flow optimizations}, has been developed to analyze the flow of data through the program and removes redundancies across these constructs.

Object-oriented programs are different from those written in many other languages, in that they consist of many more, but smaller, procedures (called \textit{methods} in object-oriented terms).

\subsection{Optimizations for Computer Architectures}

Almost all high-performance systems take advantage of the same two basic techniques: \textit{parallelism} and \textit{memory hierarchies}. Parallelism can be found at several levels: at the \textit{instruction level}, where multiple operations are executed simultaneously and at the \textit{processor level}, where different threads of the same application are run on different processors.

\section{Programming Language Basics}
\subsection{The Static/Dynamic Distinction}

If a language uses a policy that allows the compiler to decide an issue, then we say that the language uses a \textit{static} policy or that the issue can be decided at \textit{compile time}. On the other hand, a policy that only allows a decision to be made when we execute the program is said to be a \textit{dynamic policy} or to require a decision at \textit{run time}.

The \textit{scope} of a declaration of $x$ is the region of the program in which uses of $x$ refer to this declaration. A language uses \textit{static scope} or \textit{lexical scope} if it is possible to determine the scope of a declaration by looking only at the program. Otherwise, the language uses \textit{dynamic scope}.

\subsection{Environments and States}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \node (node1) at (0,0) [anchor=north] {names};
        \node (node2) at (3.5,0) [align=center, anchor=north] {locations\\(variables)};
        \node (node3) at (7,0) [anchor=north] {values};
        \draw [-latex, bend left=45] (node1.north east) to node[midway, above] {\textit{environment}} (node2.north west);
        \draw [-latex, bend left=45] (node2.north east) to node[midway, above] {\textit{state}} (node3.north west);
    \end{tikzpicture}
    \caption{Two-stage mapping from names to values}
    \label{figure:1.8}
\end{figure}

The association of names with locations in memory (the \textit{store}) and then with values can be described by two mappings that change as the program runs:
\begin{enumerate}
    \item The \textit{environment} is a mapping from names to locations in the store.
    \item The \textit{state} is a mapping from locations in store to their values.
\end{enumerate}

The environment and state mappings in Fig.\;\ref{figure:1.8} are dynamic, but there are a few exceptions:
\begin{enumerate}
    \item \textit{Static versus dynamic binding} of names to locations.
    \item \textit{Static versus dynamic binding} of locations to values.
\end{enumerate}

\begin{framed}
\begin{center}
    \textbf{{\large Names, Identifiers, and Variables}}
\end{center}

An \textit{identifier} is a string of characters, typically letters or digits, that refers to (identifies) an entity. Composite names are called \textit{qualified} names.

A \textit{variable} refers to a particular location of the store.
\end{framed}

\subsection{Static Scope and Block Structure}
\label{section:1.6.3}

The scope rules for C are based on program structure; the scope of a declaration is determined implicitly by where the declaration appears in the program. Later languages also provide explicit control over scopes through the use of keywords like \textbf{public}, \textbf{private} and \textbf{protected}.

A \textit{block} is a grouping of declarations and statements. C uses braces \verb|{| and \verb|}| to delimit a block; the alternative use of \textbf{begin} and \textbf{end} for the same purpose dates back to Algol.

In C, the syntax of blocks is given by
\begin{enumerate}
    \item One type of statement is a block. Blocks can appear anywhere that other types of statement can appear.
    \item A block is a sequence of declarations followed by a sequence of statements, all surrounded by braces.
\end{enumerate}

Note that this syntax allows blocks to be nested inside each other. This nesting property is referred to as \textit{block structure}.

\subsection{Explicit Access Control}

Through the use of keywords like \textbf{public}, \textbf{private}, and \textbf{protected}, object-oriented languages provide explicit control over access to member names in a superclass. These keywords support \textit{encapsulation} by restricting access.

\subsection{Dynamic Scope}

Technically, any scoping policy is dynamic if it is based on factor(s) that can be known only when the program executes. The term \textit{dynamic scope}, however, usually refers to the following policy: a use of a name $x$ refers to the declaration of $x$ in the most recently called procedure with such a declaration.

\begin{framed}
    \begin{center}
        \textbf{{\large Declarations and Definitions}}
    \end{center}

    In C++, a method is declared in a class definition, by giving the types of the arguments and result of the method (often called the \textit{signature} for the method).
\end{framed}

\subsection{Parameter Passing Mechanisms}

\textit{Actual parameters} (the parameters used in the call of a procedure) are associated with the \textit{formal parameters} (those used in the procedure definition).

\subsubsection{Call-by-Value}

In \textit{call-by-value}, the actual parameter is evaluated (if it is an expression) or copied (if it is a variable).

\subsubsection{Call-by-Reference}

In \textit{call-by-reference}, the address of the actual parameter is passed to the callee as the value of the corresponding formal parameter.

\subsection{Aliasing}

It is possible that two formal parameters can refer to the same location; such variables are said to be \textit{aliases} of one another.

\chapter{A Simple Syntax-Directed Translator}
\section{Introduction} 

The \textit{syntax} of a programming language defines what its programs, while the \textit{semantics} of the language defines what its program mean; that is, what each program does when it executes.

A lexical analyzer allows a translator to handle mutlicharacter constructs like identifiers, which are written as sequences of charactersm, but are treated as units called \textit{tokens} during syntax analysis.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\linewidth}
        \centering
        \begin{tikzpicture}
            \node (node1) at (0,0) {\textbf{do-while}};
            \node (node2) at (-1.5,-1) {\textbf{body}};
            \node (node3) at (1.5,-1) {\verb|>|};
            \node (node4) at (-1.5,-2) {\textbf{assign}};
            \node (node5) at (1,-2) {\verb|[]|};
            \node (node6) at (2,-2) {$v$};
            \node (node7) at (-2,-3) {$i$};
            \node (node8) at (-1,-3) {\verb|+|};
            \node (node9) at (0.5,-3) {$a$};
            \node (node10) at (1.5,-3) {$i$};
            \node (node11) at (-1.5,-4) {$i$};
            \node (node12) at (-0.5,-4) {1};
            \draw ($(node1.south west)!0.25!(node1.south east)$) to ($(node2.north east)!0.25!(node2.north west)$);
            \draw ($(node1.south east)!0.25!(node1.south west)$) to (node3.north west);
            \draw (node2.south) to (node4.north);
            \draw ($(node3.south west)!0.25!(node3.south east)$) to ($(node5.north east)!0.25!(node5.north west)$);
            \draw ($(node3.south east)!0.25!(node3.south west)$) to ($(node6.north west)!0.25!(node6.north east)$);
            \draw ($(node4.south west)!0.4!(node4.south east)$) to ($(node7.north east)!0.25!(node7.north west)$);
            \draw ($(node4.south east)!0.4!(node4.south west)$) to ($(node8.north west)!0.25!(node8.north east)$);
            \draw ($(node5.south west)!0.25!(node5.south east)$) to ($(node9.north east)!0.25!(node9.north west)$);
            \draw ($(node5.south east)!0.25!(node5.south west)$) to ($(node10.north west)!0.25!(node10.north east)$);
            \draw ($(node8.south west)!0.25!(node8.south east)$) to ($(node11.north east)!0.25!(node11.north west)$);
            \draw ($(node8.south east)!0.25!(node8.south west)$) to ($(node12.north west)!0.25!(node12.north east)$);
        \end{tikzpicture}
        \subcaption{}
    \end{minipage}
    \begin{minipage}{0.45\linewidth}
        \centering
        \begin{tabular}{l}
             \verb|1:|\quad\verb|i = i + 1|\\
             \verb|2:|\quad\verb|t1 = a [ i ]|\\
             \verb|3:|\quad\verb|if t1 < v goto 1|
        \end{tabular}
        \subcaption{}
    \end{minipage}
    \caption{Intermediate code for "\texttt{do i=i+1; while(a[i]<v);}"}
    \label{figure:2.4}
\end{figure}

Two forms of intermediate code are illustrated in Fig.\;\ref{figure:2.4}. One form, called \textit{abstract syntax trees} or simply \textit{syntax trees}, represents the hierarchical systematic structure of the source program.

\section{Syntax Definition}

A grammar naturally describes the hierarchical structure of most programming language constructs. For example, an if-else statement in Java can have the form
\begin{center}
    \textbf{if} (expression) statement \textbf{else} statement
\end{center}

Using the variable \textit{expr} to denote an expression and the variable \textit{stmt} to denote a statement, this structuring rule can be expressed as
\begin{center}
    \textit{stmt} $\rightarrow$ \textbf{if} (\textit{expr}) \textit{stmt} \textbf{else} \textit{stmt}
\end{center}
in which the arrow may be read as "can have the form." Such a rule is called a \textit{production}. In a production, lexical elements are called \textit{terminals}. Variables like \textit{expr} and \textit{stmt} represent sequences of terminals and are called \textit{nonterminals}.

\subsection{Definition of Grammars}

A \textit{context-free grammar} has four components:
\begin{enumerate}
    \item A set of \textit{terminal} symbols, sometimes referred to as "tokens."
    \item A set of \textit{nonterminals}, sometimes called "syntactic variables."
    \item A set of \textit{productions}, where each production consists of a nonterminal, called the \textit{head} or \textit{left side} of the production, an arrow, and a sequence of terminals and/or nonterminals, called the \textit{body} or \textit{right side} of the production.
    \item A designation of one of the nonterminals as the \textit{start} symbol.
\end{enumerate}

\begin{framed}
    \begin{center}
        \textbf{{\large Tokens Versus Terminals}}
    \end{center}

    A token consists of two components, a token name and an attribute value. The token names are abstract symbols that are used by the parser for syntax analysis. Often, we shall call these token names \textit{terminals}, since they appear as terminal symbols in the grammar for a programming language.
\end{framed}

We say a production is \textit{for} a nonterminal if the nonterminal is the head of the production. The string of zero terminals, written as $\epsilon$, is called the \textit{empty} string.

\subsection{Derivations}

The terminal strings that can be derived from the start symbol form the \textit{language} defined by the grammar.

\textit{Parsing} is the problem of taking a string of terminals and figuring out how to derive it from the start symbol of the grammar, and if it cannot be derived from the start symbol of the grammar, then reporting syntax errors within the string.

\subsection{Parse Trees}

A parse tree pictorially shows how the start symbol of a grammar derives a string in the language.

Formally, given a context-free grammar, a \textit{parse tree} according to the grammar is a tree with the following properties:
\begin{enumerate}
    \item The root is labeled by the start symbol.
    \item Each leaf is labeled by a terminal or by $\epsilon$.
    \item Each interior node is labeled by a nonterminal.
    \item If $A$ is the nonterminal labeling some interior node and $X_1,X_2,\cdots,X_n$ are the labels of the children of that node from left to right, then there must be a production $A\rightarrow X_1X_2\cdots X_n$.
\end{enumerate}

\begin{framed}
    \begin{center}
        \textbf{{\large Tree Terminology}}
    \end{center}

    Tree data structures figure prominently in compiling.
    \begin{itemize}
        \item A tree consists of one or more \textit{nodes}. Nodes may have \textit{labels}.
        \item Exactly one node is the \textit{root}. All nodes except the root have a unique \textit{parent}; the root has no parent.
        \item If node $N$ is the parent of node $M$, then $M$ is a \textit{child} of $N$. The children of one node are called \textit{siblings}. They have an order, \textit{from the left}, and when we draw trees, we order the children 
        \item A node with no children is called a \textit{leaf}. Other nodes -- those with one or more children -- are \textit{interior nodes}.
        \item A \textit{descendant} of a node $N$ is either $N$ itself, a child of $N$, a child of a child of $N$, and so on, for any number of levels. We say node $N$ is an \textit{ancestor} of node $M$ if $M$ is a descendant of $N$.
    \end{itemize}
\end{framed}

From left to right, the leaves of a parse tree form the \textit{yield} of the tree, which is the string \textit{generated} or \textit{derived} from the nonterminal at the root of the parse tree.

The process of finding a parse tree for a given string of terminals is called \textit{parsing} that string.

\subsection{Ambiguity}

We have to be careful in talking about \textit{the} structure of a string according to a grammar. A grammar can have more than one parse tree generating a given string of terminals. Such a grammar is said to be \textit{ambiguous}.

\subsection{Associativity of Operators}

We say that the operator $+$ \textit{associates} to the left, because an operand with plus signs on both sides of it belongs to the operator to its left.

\subsection{Precedence of Operators}

We say that $*$ has \textit{higher precedence} than $+$ if $*$ takes its operands before $+$ does.

\section{Syntax-Directed Translation}

This section introduces two concepts related to syntax-directed translation:
\begin{itemize}
    \item\textit{Attributes}. An \textit{attribute} is any quantity associated with a programming construct.
    \item(\textit{Syntax-directed}) \textit{translation schemes}. A \textit{translation scheme} is a notation for attaching program fragments to the productions of a grammar.
\end{itemize}

\subsection{Postfix Notation}

The \textit{postfix notation} for an expression $E$ can be defined inductively as follows:
\begin{enumerate}
    \item If $E$ is a variable or constant, then the postfix notation for $E$ is $E$ itself.
    \item If $E$ is an expression of the form $E_1$ \textbf{op} $E_2$, where \textbf{op} is any binary operator, then the postfix notation for $E$ is $E_1'$ $E_2'$ \textbf{op}, where $E_1'$ and $E_2'$ are the postfix notations for $E_1$ and $E_2$, respectively.
    \item If $E$ is a parenthesized expression of the form $(E_1)$, then the postfix notation for $E$ is the same as the postfix notation for $E_1$.
\end{enumerate}

No parentheses are needed in postfix notation, because the position and \textit{arity} (number of arguments) of the operators permits only one decoding of a postfix expression.

\subsection{Synthesized Attributes}

A \textit{syntax-directed definition} associates:
\begin{enumerate}
    \item With each grammar symbol, a set of attributes, and
    \item With each production, a set of \textit{semantic rules} for computing the values of the attributes associated with the symbols appearing in the production.
\end{enumerate}

A parse tree showing the attribute values at each node is called an \textit{annotated} parse tree.

An attribute is said to be \textit{synthesized} if its value at a parse-tree node $N$ is determined from attribute values at the children of $N$ and at $N$ itself.

\subsection{Tree Traversals}

A \textit{traversal} of a tree starts at the root and visits each node of the tree in some order.

A \textit{depth-first} traversal starts at the root and recursively visits the children of each node in any order, not necessarily from left to right.

Synthesized attributes can be evaluated during any \textit{bottom-up} traversal, that is, a traversal that evaluates attributes at a node after having evaluated attributes at its children.

\subsection{Translation Schemes}

\begin{framed}
    \begin{center}
        \textbf{{\large Preorder and Postorder Traversals}}
    \end{center}

    Often, we traverse a tree to perform some particular action at each node. If the action is done when we first visit a node, then we may refer to the traversal as a \textit{preorder traversal}. Similarly, if the action is done just before we leave a node for the last time, then we say it is a \textit{postorder traversal} of the tree.

    The \textit{preorder} of a (sub)tree rooted at node $N$ consists of $N$, followed by the preorders of the subtrees of each of its children, if any, from the left. The \textit{postorder} of a (sub)tree rooted at $N$ consists of the postorders of each of the subtrees for the children of $N$, if any, from the left, followed by $N$ itself.
\end{framed}

Program fragments embedded within production bodies are called \textit{semantic actions}.

\section{Parsing}

Most parsing methods fall into one of two classes, called the \textit{top-down} and \textit{bottom-up} methods.

\subsection{Top-Down Parsing}

The current terminal being scanned in the input is frequently referred to as the \textit{lookahead} symbol.

\subsection{Predictive Parsing}

\textit{Recursive-descent parsing} is a top-down method of syntax analysis in which a set of recursive procedures is used to process the input. Here, we consider a simple form of recursive-descent parsing, called \textit{predictive parsing}, in which the lookahead symbol unambiguously determines the flow of control through the procedure body for each nonterminal.

\subsection{Designing a Predictive Parser}

Recall that a \textit{predictive parser} is a program consisting of a procedure for every nonterminal.

\subsection{Left Recursion}

Consider a nonterminal $A$ with tow productions $$A\;\rightarrow\;A\alpha\;|\;\beta$$ where $\alpha$ and $\beta$ are sequences of terminals and nonterminals that do not start with $A$.

The nonterminal $A$ and its production are said to be \textit{left recursive}, because the production $A\to A\alpha$ has $A$ itself as the leftmost symbol on the right side.

The same effect can be achieved by rewriting the productions for $A$ in the following manner, using a new nonterminal $R$:
\begin{align*}
    &A\;\to\;\beta R\\
    &R\;\to\;\alpha R\;|\;\epsilon
\end{align*}

Nonterminal $R$ and its production $R\to\alpha R$ are \textit{right recursive} because this production for $R$ has $R$ itself as the last symbol on the right side.

\section{A Translator for Simple Expressions}

\subsection{Abstract and Concrete Syntax}

In an \textit{abstract syntax tree} for an expression, each interior node represents an operator; the children of the node represent the operands of the operator.

Abstract syntax trees, or simply \textit{syntax trees}, resemble parse trees to an extent. Many nonterminals of a grammar represent programming constructs, but others are "helpers" of one sort of another. In the syntax tree, these helpers typically are not needed and are hence droped. To emphasize the contrast, a parse tree is sometimes called a \textit{concrete syntax tree}, and the underlying grammar is called a \textit{concrete syntax} for the language.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \node (node1) at (0,0) {\verb|+|};
        \node (node2) at (-1,-1) {\verb|-|};
        \node (node3) at (1,-1) {\verb|2|};
        \node (node4) at (-2,-2) {\verb|9|};
        \node (node5) at (0,-2) {\verb|5|};
        \draw (node1.south west) to (node2.north east);
        \draw (node1.south east) to (node3.north west);
        \draw (node2.south west) to (node4.north east);
        \draw (node2.south east) to (node5.north west);
    \end{tikzpicture}
    \caption{Syntax tree for \texttt{9-5+2}}
    \label{Figure:2.22}
\end{figure}

In the syntax tree in Fig.\;\ref{Figure:2.22}, each interior node is associated with an operator, with no "helper" nodes for \textit{single productions} (a production whose body consists of a single nonterminal, and nothing else) or for $\epsilon$-productions.

\subsection{Simplifying the Translator}

When the last statement executed in a procedure body is a recursive call to the same procedure, the call is said to be \textit{tail recursive}.

\subsection{The Complete Program}

\begin{Verbatim}
    import java.io.*
    class Parser {
        static int lookahead;

        public Parser() throws IOException {
            lookahead = System.in.read();
        }

        void expr() throws IOException {
            term();
            while(true) {
                if (lookahead == '+') {
                    match('+'); term(); System.out.write('+');
                }
                else if (lookahead == '-') {
                    match('-'); term(); System.out.write('-');
                }
                else return;
            }
        }

        void term() throws IOException {
            if (Character.isDigit((char)lookahead)) {
                System.out.write((char)lookahead); match(lookahead);
            }
            else throw new Error("syntax error");
        }
    }

    public class Postfix {
        public static void main(String[] args) throws IOException {
            Parser parse = new Parser();
            parse.expr(); System.out.write('\n');
        }
    }
\end{Verbatim}
\begin{figure}[htbp]
    \caption{Java program to translate infix expressions into postfix form}
    \label{Figure:2.27}
\end{figure}

The function \verb|Parser|, with the same name as its class, is a \textit{constructor}; it is called automatically when an object of the class is created.

The construction \verb|(char)lookahead| \textit{casts} or coerces \verb|lookahead| to be a character.

\section{Lexical Analysis}

A sequence of input characters that comprises a single token is called a \textit{lexeme}.

\subsection{Recognizing Keywords and Identifiers}

Most languages use fixed character strings as punctuation marks or to identify consturcts. Such character strings are called \textit{keywords}.

Grammars routinely treat identifiers as terminals to simplify the parser, which can then expect the same terminal, say \textbf{id}, each time any identifier appears in the input. For example, on input
\begin{equation}
    \verb|count = count + increment;|
\end{equation}
the parser works with the terminal stream \textbf{id}\verb|=|\textbf{id}\verb|+|\textbf{id}.

Keywords generally satisfy the rules for forming identifiers, so a mechanism is needed f deciding when a lexeme forms a keyword and when it forms an identifier. The problem is easier to resolve if keywords are \textit{reserved}; i.e., if they cannot be used as identifiers.

The lexical analyzer in this section solves two problems by using a table to hold character strings:
\begin{itemize}
    \item\textit{Single Representation.}
    \item\textit{Reserved Words}.
\end{itemize}

In Java, a string table can be implemented as a hash table using a class called \textit{Hashtable}.

\section{Symbol Tables}

\textit{Symbol tables} are data structures that are used by compilers to hold information about source-program constructs.

A program consists of blocks with optional declarations and "statements" consisting of single identifiers. Each such statement represents a use of the identifier. Here is a sample program in this language:
\begin{equation}
    \verb|{ int x; char y; { bool y; x; y; } x; y; }|
    \label{2.7}
\end{equation}

\subsection{Symbol Table Per Scope}

The term \textit{scope} by itself refers to a portion of a program that is the scope of one or more declarations.

The \textit{most-closely nested} rule for blocks is that an identifier $x$ is in the scope of the most-closely nested declaration of $x$; that is, the declaration of $x$ found by examining blocks inside-out, starting with the block in which $x$ appears.

\section{Intermediate Code Generation}
\subsection{Two Kinds of Intermediate Representations}

In addition to creating an intermediate representation, a compiler front end checks that the source program follows the syntactic and semantic rules of the source language. This checking is called \textit{static checking}; in general "static" means "done by  the compiler."

\subsection{Construction of Syntax Trees}
\subsubsection{Syntax Trees for Expressions}

The table in Fig.\;\ref{Figure:2.41}  specifies the correspondence between the concrete and abstract syntax for several of the operators of Java.

The subscript \textit{unary} in $\verb|-|_{unary}$ is solely to distinguish a leading unary minus sign from a binary minus sign.

\begin{equation*}
    \begin{array}{cc}\text{Concrete Syntax}&\text{Abstract Syntax}\\\verb|=| &\textbf{assign}\\\verb|||| &\textbf{cond}\\\verb|&&|&\textbf{cond}\\\verb|== !=|&\textbf{rel}\\\verb|< <= >= >|&\textbf{rel}\\\verb|+ -|&\textbf{op}\\\verb|* / %|&\textbf{op}\\\verb|!|&\textbf{not}\\\verb|-|_{unary}&\textbf{minus}\\\verb|[]|&\textbf{access}\end{array}
\end{equation*}
\begin{figure}[htbp]
    \caption{Concrete and abstract syntax for several Java operators}
    \label{Figure:2.41}
\end{figure}

\subsection{Static Checking}

Static checking includes:
\begin{itemize}
    \item\textit{Syntactic Checking}.
    \item\textit{Type Checking}.
\end{itemize}

\subsubsection{L-values and R-values}

The terms \textit{l-value} and \textit{r-value} refer to values that are appropriate on the left and right sides of an assignment, respectively.

\subsubsection{Type Checking}

Type checking assures that the type of a construct matches that expected by its context.

The idea of matching actual with expected types continues to apply, even in the following situations:
\begin{itemize}
    \item\textit{Coercions}. A \textit{coercion} occurs if the type of an operand is automatically converted to the type expected by the operator.
    \item\textit{Overloading}. A symbol is said to be \textit{overloaded} if it has different meanings depending on its context.
\end{itemize}

\subsection{Three-Address Code}
\subsubsection{Better Code for Expressions}

We can avoid some copy instructions by modifying the translation functions to generate a partial instruction that computes, say \verb|j+k|, but does not commit to where the result is to be placed, signified by \textbf{null} address for the result:
\begin{equation}
    \textbf{null}\verb| = j + k|
\end{equation}

\chapter{Lexical Analysis}

To implement a lexical analyzer by hand, it helps to start with a diagram or other description for the lexemes of each token. We can then write code to identify each occurrence of each lexeme on the input and to return information about the token identified.

We can also produce a lexical analyzer automatically by specifying the lexeme patterns to a \textit{lexical-analysis generator} and compiling those patterns into code that functions as a lexical analyzer.

\section{The Role of the Lexical Analyzer}

Since the lexical analyzer is the part of the compiler that reads the source text, it may perform certain other tasks besides identification of lexemes. One such task is stripping out comments and \textit{whitespace} (blank, newline, tab, and perhaps other characters that are used to separate tokens in the input).

Sometimes, lexical analyzers are divided into a cascade of two processes:
\begin{itemize}
    \item[a)]\textit{Scanning} consists of the simple processes that do not require tokenization of the input.
    \item[b)]\textit{Lexical analysis} proper is the more complex portion, where the scanner produces the sequence of tokens as output.
\end{itemize}

\subsection{Tokens, Patterns, and Lexemes}

When discussing lexical analysis, we use three related but distinct terms:
\begin{itemize}
    \item A \textit{token} is a pair consisting of a token name and an optional attribute value.
    \item A \textit{pattern} is a description of the form that the lexemes of a token may take. In the case of a keyword as a token, the pattern is just the sequence of characters that form the keyword. For identifiers and some other tokens, the pattern is a more complex structure that is \textit{matched} by many strings.
    \item A \textit{lexeme}  is a sequence of characters in the source program that matches the pattern for a token and is identified by the lexical analyzer as an instance of that token.
\end{itemize}

\section{Specification of Tokens}

\begin{framed}
    \begin{center}
        \textbf{{\large Can We Run Out of Buffer Space}}
    \end{center}

    To avoid problems with long character strings, we can treat them as a concatenation of components, one from each line over which the string is written.

    A more difficult problem occurs when arbitrarily long lookahead may be needed. For example, some languages like PL/I do not treat keywords as \textit{reserved}; that is, you can use identifiers with the same name as a keyword.
\end{framed}

\subsection{Strings and Languages}

An \textit{alphabet} is any finite set of symbols. The set $\{0,1\}$ is the \textit{binary alphabet}.

A \textit{string} over an alphabet is a finite sequence of symbols drawn from that alphabet. The \textit{empty string}, denoted $\epsilon$, is the string of length zero.

A \textit{language} is any countable set of strings over some fixed alphabet. Abstract languages like $\emptyset$, the \textit{empty set} are languages under this definition.

\begin{framed}
    \begin{center}
        \textbf{{\large Terms for Parts of Strings}}
    \end{center}

    The following string-related terms are commonly used:
    \begin{enumerate}
        \item A \textit{prefix} of string $s$ is any string obtained by removing zero or more symbols from the end of $s$.
        \item A \textit{suffix} of string $s$ is any string obtained by removing zero or more symbols from the beginning of $s$.
        \item A \textit{substring} of $s$ is obtained by deleting any prefix and any suffix from $s$.
        \item The \textit{proper} prefixes, suffixes, and substrings of a string $s$ are those, prefixes, suffixes, and substrings, respectively, of $s$ that are not $\epsilon$ or not equal to $s$ itself.
        \item A \textit{subsequence} of $s$ is any string formed by deleting zero or more not necessarily consecutive positions of $s$.
    \end{enumerate}
\end{framed}

If $x$ and $y$ are strings, then the \textit{concatenation} of $x$ and $y$, denoted $xy$, is the string formed by appending $y$ to $x$.

\subsection{Operations and Languages}

The (\textit{Kleene}) \textit{closure} of a language $L$, denoted $L^*$, is the set of strings you get by concatenating $L$ zero or more times.

\subsection{Regular Expressions}

A language that can be defined by a regular expression is called a \textit{regular set}. If two regular expressions $r$ and $s$ denote the same regular set, we say they are \textit{equivalent} and write $r=s$.

\subsection{Regular Definitions}
\label{Section:3.3.4}

If $\Sigma$ is an alphabet of basic symbols, then a \textit{regular definition} is a sequence of definitions of the form: $$\begin{array}{ccc}d_1&\to&r_1\\d_2&\to&r_2\\&\cdots\\d_n&\to&r_n\end{array}$$ where:
\begin{enumerate}
    \item Each $d_i$ is a new symbol, not in $\Sigma$ and not the same as any other of the $d$'s, and
    \item Each $r_i$ is a regular expression over the alphabet $\Sigma\cup\{d_1,d_2,\ldots,d_{i-1}\}$.
\end{enumerate}

\subsection{Extensions of Regular Expressions}

Here we mention a few notational extensions that were first incorporated into Unix utilities that are particularly useful in the specification lexical analyzer.
\begin{enumerate}
    \item\textit{One or more instances}.
    \item\textit{Zero or one instance}.
    \item\textit{Character classes}.
\end{enumerate}

\section{Recognition of Tokens}
\subsection{Transition Diagrams}

\textit{Transition diagrams} have a collection of nodes or circles, called \textit{states}.

\textit{Edges} are directed from one state of the transition diagram to another. Each edge is \textit{labeled} by a symbol or set of symbols. We shall assume that all our transition diagrams are \textit{deterministic}, meaning that there is never more than one edge out of a given state with a given symbol among its labels. Some important conventions about transition diagrams are:
\begin{itemize}
    \item Certain states are said to be \textit{accepting}, or \textit{final}.
    \item One state is designated the \textit{start state}, or \textit{initial state}; it is indicated by an edge, labeled "start," entering from nowhere.
\end{itemize}

\subsection{Recognition of Reserved Words and Identifiers}

Usually, keywords are reserved, so they are not identifiers even though they \verb|look| like identifiers.

\section{The Lexical-Analyzer Generator \texttt{Lex}}

The input notation for the \verb|Lex| tool is referred to as the \textit{Lex language} and the tool itself is the \textit{Lex compiler}.

\subsection{Structure of \texttt{Lex} Programs}

A \verb|Lex| program has the following form:
\begin{equation*}
    \begin{aligned}&\text{declarations}\\&\%\%\\&\text{transition rules}\\&\%\%\\&\text{auxiliary functions}\end{aligned}
\end{equation*}
The declarations section includes declarations of variables, \textit{manifest constants} (identifiers declared to stand for a constant), and regular definitions, in the style of Section 3.\ref{Section:3.3.4}.

\section{Finite Automata}
We shall now discover how \verb|Lex| turns its input program into a lexical analyzer. At the heart of the transition is the formalism known as \textit{finite automata}. These are essentially graphs, like transition diagrams, with a few differences:
\begin{enumerate}
    \item Finite automata are \textit{recognizers};  they simply say "yes" or "no" about each possible input string.
    \item Finite automata come in two flavors:
    \begin{itemize}
        \item[(a)]\textit{Nondeterministic finite automata} (NFA) have no restrictions on the labels of their edges.
        \item[(b)]\textit{Deterministic finite automata} (DFA) have, for each state, and for each symbol of its input alphabet exactly one edge with that symbol leaving that state.
    \end{itemize}
\end{enumerate}

Both deterministic and nondeterministic finite automata are capable of recognizing the same languages. In fact these languages are exactly the same languages, called the \textit{regular languages}, the regular expressions can describe.\footnote{There is a small lacuna: as we defined them, regular expressions cannot describe the empty language, since we would never want to use this pattern in practice. However, finite automata \textit{can} define the empty language.}

\subsection{Nondeterministic Finite Automata}

A \textit{nondeterministic finite automata} (NFA) consists of:
\begin{enumerate}
    \item A finite set of state $S$.
    \item A set of input symbols $\Sigma$, the \textit{input alphabet}.
    \item A \textit{transition function} that gives, for each state, and for each symbol in $\Sigma\cup\{\epsilon\}$ a set of \textit{next states}.
    \item A state $s_0$ from $S$ that is distinguished as the \textit{start state} (or \textit{initial state}).
    \item A set of states $F$, a subset of $S$, that is distinguished as the \textit{accepting states} (or \textit{final states}).
\end{enumerate}
 
We can represent either an NFA or DFA by a \textit{transition graph}, where the nodes are states and the labeled edges represent the transition function.

\subsection{Transition Tables}

We can also represent an NFA by a \textit{transition table}, whose rows correspond to states, and whose columns correspond to the input symbols and $\epsilon$.

\subsection{Acceptance of Input Strings by Automata}

An NFA \textit{accepts} input string $x$ if and only if there is some path in the transition graph from the start state to one of the accepting staets, such that the symbols along the path spell out $x$.

The \textit{language defined} (or \textit{accepted}) by an NFA is the set of strings labeling some path from the start to an accepting state.

\subsection{Deterministic Finite Automata}

A \textit{deterministic finite automata} (DFA) is a special case of an NFA where:
\begin{enumerate}
    \item There are no moves on input $\epsilon$, and
    \item For each state $s$ and input symbol $a$, there is exactly one edge out of $s$ labeled $a$.
\end{enumerate}

\exm{Simulating a DFA.}{
    \noindent{\small\textbf{INPUT:}} An input string $x$ terminated by an end-of-file character \textbf{eof}. A DFA $D$ with start state $s_0$, accepting states $F$, and transition function $move$.

    \noindent{\small\textbf{OUTPUT:}} Answer "yes" if $D$ accepts $x$; "no" otherwise.

    \noindent{\small\textbf{METHOD:}} Apply the algorithm in Fig.\;\ref{Figure:3.27} to the input string $x$. The function $move(s,c)$ gives the state to which there is an edge from state $s$ on input $c$. The function $nextChar$ returns the next character of the input string $x$.
}

\begin{center}
    \begin{tabular}{l}
        $s=s_0$;\\
        $c=nextChar()$;\\
        \textbf{while} ( $c$ !$=$ \textbf{eof} ) \{\\
        \qquad $s=move(s,c)$;\\
        \qquad $c=nextChar()$;\\
        \}\\
        \textbf{if} ( $s$ is in $F$ ) \textbf{return} \verb|"yes"|;\\
        \textbf{else return} \verb|"no"|;
    \end{tabular}
\end{center}
\begin{figure}[htbp]
    \caption{Simulating a DFA}
    \label{Figure:3.27}
\end{figure}

\section{From Regular Expressions to Automata}
\subsection{Conversion of an NFA to a DFA}

\exm{The \textit{subset construction} of a DFA from an NFA.}{
    \noindent{\small\textbf{INPUT:}} An NFA $N$.

    \noindent{\small\textbf{OUTPUT:}} A DFA $D$ accepting the same language as $N$.

    \noindent{\small\textbf{METHOD:}} Our algorithm constructs a transition table $Dtran$ for $D$. Each state of $D$ is a set of NFA states, and we construct $Dtran$ so $D$ will simulate "in parallel" all possible moves $N$ can make on a given input string. Our first problem is to deal with $\epsilon$-transitions of $N$ properly. In Fig.\;\ref{Figure:3.31} we see the definitions of several functions that describe basic computations on the states of $N$ that are needed in the algorithm. Note that $s$ is a single state of $N$, while $T$ is a set of states of $N$.

    We must explore those sets of states that $N$ can be in after seeing some input string. As a basis, before reading the first input symbol, $N$ can be in any of the states of $\epsilon$-$closure(s_0)$, where $s_0$ is its start state. For the induction, suppose that $N$ can be in set of states $T$ after reading input string $x$. If it next reads input $a$, then $N$ can immediately go to any of the states in $move(T,a)$. However, after reading $a$, it may also make several $\epsilon$-transitions; thus $N$ could be in any state of $\epsilon$-$closure(move(T,a))$ after reading input $xa$. Following these ideas, the construction of the set of $D$'s states, $Dstates$, and its transition function $Dtran$, is shown in Fig.\;\ref{Figure:3.32}.
    
    The start state of $D$ is $\epsilon$-$closure(s_0)$, and the accepting states of $D$ are all those sets of $N$'s states that include at least one accepting state of $N$. To complete our description of the subset construction, we need only to show how initially, $\epsilon$-$closure(s_0)$ is the only state in $Dstates$, and it is unmarked; $\epsilon$-$closure(T)$ is computed for any set of NFA states $T$. This process, shown in Fig.\;\ref{Figure:3.33}, is a straightforward search in graph from a set of states. In this case, imagine that only the $\epsilon$-labeled edges are available in the graph.
}

\begin{tabular}{p{0.15\columnwidth}|p{0.75\columnwidth}}
    \hline
    \multicolumn{1}{c|}{Operation} & \multicolumn{1}{c}{Description} \\ \hline
    $\epsilon$-$closure(s)$ & Set of NFA states reachable from NFA state $s$ on $\epsilon$-transitions alone. \\ \hline
    $\epsilon$-$closure(T)$ & Set of NFA states reachable from NFA state $s$ in set $T$ on $\epsilon$-transitions alone; $=\cup_{s\text{ in }T}\epsilon$-$closure(s)$. \\ \hline
    $move(T,a)$ & Set of NFA states to which there is transition on input symbol $a$ from some table $s$ in $T$. \\ \hline
\end{tabular}
\begin{figure}[htbp]
    \caption{Operations on NFA states}
    \label{Figure:3.31}
\end{figure}

\begin{center}
    \begin{tabular}{l}
        \textbf{while} ( there is an unmarked state $T$ in $Dstates$ ) \{\\
        \qquad mark $T$;\\
        \qquad\textbf{for} ( each input symbol $a$ ) \{\\
        \qquad\qquad$U=\epsilon$-$closure(move(T,a))$;\\
        \qquad\qquad\textbf{if} ( $U$ is not in $Dstates$ )\\
        \qquad\qquad\qquad add $U$ as an unmarked state to $Dstates$;\\
        \qquad\qquad$Dtran[T,a]=U$;\\
        \qquad\}\\
        \}
    \end{tabular}
\end{center}
\begin{figure}[htbp]
    \caption{The subset construction}
    \label{Figure:3.32}
\end{figure}

\begin{center}
    \begin{tabular}{l}
        push all states of $T$ onto $stack$;\\
        initialize $\epsilon$-$closure(T)$ to $T$;\\
        \textbf{while} ( $stack$ is not empty ) \{\\
        \qquad pop $t$, the top element, off $stack$;\\
        \qquad\textbf{for} ( each state $u$ with an edge from $t$ to $u$ labeled $\epsilon$ )\\
        \qquad\qquad\textbf{if} ( $u$ is not in $\epsilon$-$closure(T)$ ) \{\\
        \qquad\qquad\qquad add $u$ to $\epsilon$-$closure(T)$;\\
        \qquad\qquad\qquad push $u$ onto $stack$;\\
        \qquad\qquad\}\\
        \}
    \end{tabular}
\end{center}
\begin{figure}[htbp]
    \caption{Computing $\epsilon$-$closure(T)$}
    \label{Figure:3.33}
\end{figure}

\subsection{Simulation of an NFA}

\exm{Simulating an NFA.}{
    \noindent{\small\textbf{INPUT:}} An input string $x$ terminated by an end-of-file character \textbf{eof}. An NFA $N$ with start state $s_0$, accepting states $F$, and transition function $move$.

    \noindent{\small\textbf{OUTPUT:}} Answer "yes" if $M$ accepts $x$; "no" otherwise.

    \noindent{\small\textbf{METHOD:}} The algorithm keeps a set of current states $S$, those that are reached from $s_0$ following a path labeled by the inputs read so far. If $c$ is the next input character, read by function $nextChar()$, then we first compute $move(S,c)$ and then close that set using $\epsilon-closure()$. The algorithm is sketched in Fig\;\ref{Figure:3.37}.
}

\begin{center}
    \begin{tabular}{l}
        1)\quad$S=\epsilon$-$closure(s_0)$;\\
        2)\quad$c=nextChar()$;\\
        3)\quad\textbf{while} ( $c$ !$=$ \textbf{eof} ) \{\\
        4)\quad\qquad$S=\epsilon$-$closure(move(S,c))$;\\
        5)\quad\qquad$c=nextChar()$;\\
        6)\quad\}\\
        7)\quad\textbf{if} ( $S\cap F$ !$=\varnothing$ ) \textbf{return} \verb|"yes"|;\\
        8)\quad\textbf{else return} \verb|"no"|;
    \end{tabular}
\end{center}
\begin{figure}[htbp]
    \caption{Simulating an NFA}
    \label{Figure:3.37}
\end{figure}

\subsection{Efficiency of NFA Simulation}

\begin{framed}
    \begin{center}
        \textbf{{\large Big-Oh Notation}}
    \end{center}

    Technically, we say a function $f(n)$, perhaps the running time of some step of an algorithm, is $O(g(n))$  if there are constants $c$ and $n_0$, such that whenever $n\ge n_0$, it is true that $f(n)\le cg(n)$. The use of this \textit{big-oh notation} enables us to avoid getting too far into the details of what we count as a unit of execution time, yet lets us express the rate at which the running time of an algorithm grows.
\end{framed}

\subsection{Construction of an NFA from a Regular Expression}

\exm{The McNaughton-Yamada-Thompson algorithm to convert a regular expression to an NFA.}{
    \noindent{\small\textbf{INPUT:}} A regular expression $r$ over alphabet $\Sigma$.

    \noindent{\small\textbf{OUTPUT:}} An NFA $N$ accepting $L(r)$.

    \noindent{\small\textbf{METHOD:}} Begin by parsing $r$ into its constituent subexpressions. The rules for constructing an NFA consist of basic rules for handling subexpressions with no operators, and inductive rules for constructing larget NFA's from the NFA's for the immediate subexpressions of a given expression.

    \noindent{\small\textbf{BASIS:}} For expression $\epsilon$ construct the NFA
    \begin{center}
        \begin{tikzpicture}
            \node (node1) at (-0.5,0) {$i$};
            \node (node2) at (2,0) {$f$};
            \draw (-0.5,0) circle [radius=0.25];
            \draw (2,0) circle [radius=0.25];
            \draw (2,0) circle [radius=0.4];
            \draw [-latex] (-0.25,0) -- (1.6,0) node[midway, above] {$\varepsilon$};
            \draw [-latex] (-2,0) -- (-0.75,0) node[midway, above] {start};
        \end{tikzpicture}
    \end{center}
    Here, $i$ is a new state, the start state of this NFA, and $f$ is another new state, the accepting state for the NFA.

    For any subexpression $a$ in $\Sigma$, construct the NFA
    \begin{center}
        \begin{tikzpicture}
            \node (node1) at (-0.5,0) {$i$};
            \node (node2) at (2,0) {$f$};
            \draw (-0.5,0) circle [radius=0.25];
            \draw (2,0) circle [radius=0.25];
            \draw (2,0) circle [radius=0.4];
            \draw [-latex] (-0.25,0) -- (1.6,0) node[midway, above] {a};
            \draw [-latex] (-2,0) -- (-0.75,0) node[midway, above] {start};
        \end{tikzpicture}
    \end{center}
    where again $i$ and $f$ are new states, the start and accepting states, respectively. Note that in both of the basis constructions, we construct a distinct NFA, with new states, for every occurrence of $\epsilon$ or some $a$ as a subexpression of $r$.

    \noindent{\small\textbf{INDUCTION:}} Suppose $N(s)$ and $N(t)$ are NFA's for regular expression $s$ and $t$, respectively.
    \begin{itemize}
        \item[a)] Suppose $r=s|t$. Then $N(r)$, the NFA for $r$, is constructed as in Fig.\;\ref{Figure:3.40}. Here, $i$ and $f$ are new states, the start and accepting states of $N(r)$, respectively. There are $\epsilon$-transitions from $i$ to the start states of $N(s)$ and $N(t)$, and each of their accepting states have $\epsilon$-transitions to the accepting state $f$. Note that the accepting states of $N(s)$ and $N(t)$ are not accepting in $N(r)$. Since any path from $i$ to $f$ must pass through either $N(s)$ or $N(t)$ exclusively, and since the label of that path is not changed by the $\epsilon$'s leaving $i$ or entering $f$, we conclude that $N(r)$ accepts $L(s)\cup L(t)$, which is the same as $L(r)$. That is, Fig.\;\ref{Figure:3.40} is a correct construction for the union operator.
        \item[b)] Suppose $r=st$. Then construct $N(r)$ as in Fig.\;\ref{Figure:3.41}. The start state of $N(s)$ becomes the start state of $N(r)$, and the accepting state of $N(t)$ is the only accepting state of $N(r)$. The accepting state of $N(s)$ and the start state of $N(t)$ are merged into a single state, with all the transitions in or out of either state. A path from $i$ to $f$ in Fig.\;\ref{Figure:3.41} must go first through $N(s)$, and therefore its label will begin with some string in $L(s)$. The path then continues through $N(t)$, so the path's label finishes with a string in $L(t)$. As we shall soon argue, accepting states never have edges out and start states never have edges in, so it is not possible for a path to re-enter $N(s)$ after leaving it. Thus $N(r)$ accepts exactly $L(s)L(t)$, and it is a correct NFA for $r=st$.
        \item[c)] Suppose $r=s^*$. Then for $r$ we construct the NFA $N(r)$ shown in Fig.\;\ref{Figure:3.42}. Here, $i$ and $f$ are new states, the start state and lone accepting state of $N(r)$. To get from $i$ to $f$, we can either follow the introduced path labeled $\epsilon$, which takes care of the one string in $L(s)^0$, or we can go to the start state of $N(s)$, through that NFA, then from its accepting state back to its start state zero or more times. These options allow $N(r)$ to accept all the strings in $L(s)^1$, $L(s)^2$, and so on, so the entire set of strings accepted by $N(r)$ is $L(s^*)$.
        \item[d)] Finally, suppose $r=(s)$. Then $L(r)=L(s)$, and we can use the NFA $N(s)$ as $N(r)$.
    \end{itemize}
}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \node (node1) at (0,0) {$i$};
        \node (node2) at (4,0) {$f$};
        \node (node3) at (2,1) {$N(s)$};
        \node (node4) at (2,-1) {$N(t)$};
        \draw (0,0) circle [radius=0.25];
        \draw (4,0) circle [radius=0.25];
        \draw (4,0) circle [radius=0.4];
        \draw (1,1) circle [radius=0.25];
        \draw (1,-1) circle [radius=0.25];
        \draw [-latex] ({0.25*cos(45)},{0.25*sin(45)}) to node[midway,above left] {$\varepsilon$} ({1-0.25*cos(45)},{1-0.25*sin(45)});
        \draw [-latex] ({0.25*cos(45)},{-0.25*sin(45)}) to node[midway, below left] {$\varepsilon$} ({1-0.25*cos(45)},{-1+0.25*sin(45)});
        \draw (2,1) ellipse [x radius=1.5, y radius=0.5];
        \draw (2,-1) ellipse [x radius=1.5, y radius=0.5];
        \draw (3,1) circle [radius=0.25];
        \draw (3,-1) circle [radius=0.25];
        \draw [-latex] (-1.25,0) -- (-0.25,0) node[midway, above] {start};
        \draw [-latex] ({3+0.25*cos(45)},{1-0.25*sin(45)}) to node[midway, above right] {$\varepsilon$} ({4-0.4*cos(45)},{0.4*sin(45)});
        \draw [-latex] ({3+0.25*cos(45)},{-1+0.25*sin(45)}) to node[midway, below right] {$\varepsilon$} ({4-0.4*cos(45)},{-0.4*sin(45)});
    \end{tikzpicture}
    \caption{NFA for the union of two regular expressions}
    \label{Figure:3.40}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \node (node1) at (0,0) {$i$};
        \node (node2) at (1,0) {$N(s)$};
        \node (node3) at (3,0) {$N(t)$};
        \node (node4) at (4,0) {$f$};
        \draw (0,0) circle [radius=0.25];
        \draw [-latex] (-1.5,0) to node[midway, above] {start} (-0.5,0);
        \draw (1,0) ellipse [x radius=1.5, y radius=0.5];
        \draw (2,0) circle [radius=0.25];
        \draw (3,0) ellipse [x radius=1.5, y radius=0.5];
        \draw (4,0) circle [radius=0.25];
        \draw (4,0) circle [radius=0.35];
    \end{tikzpicture}
    \caption{NFA for the concatenation of two regular expressions}
    \label{Figure:3.41}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \node (node1) at (0,0) {$i$};
        \node (node2) at (2.25,0) {$N(s)$};
        \node (node3) at (4.5,0) {$f$};
        \draw (0,0) circle [radius=0.25];
        \draw [-latex] (-1.25,0) to node[midway, above] {start} (-0.25,0);
        \draw (1.5,0) circle [radius=0.25];
        \draw (3,0) circle [radius=0.25];
        \draw (4.5,0) circle [radius=0.25];
        \draw (4.5,0) circle [radius=0.4];
        \draw (2.25,0) ellipse [x radius=1.25, y radius=0.5];
        \draw [-latex] (0.25,0) to node[midway, above] {$\varepsilon$} (1.25,0);
        \draw [-latex] (3.25,0) to node[midway, above] {$\varepsilon$} (4.1,0);
        \draw [-latex, bend right=60] (3,0.25) to node[midway, above] {$\varepsilon$} (1.5,0.25);
        \draw [-latex, bend right=45] ({0.25*cos(45)},{-0.25*sin(45)}) to node[midway, below] {$\varepsilon$} ({4.5-0.4*cos(45)},{-0.4*sin(45)});
    \end{tikzpicture}
    \caption{NFA for the closure of a regular expression}
    \label{Figure:3.42}
\end{figure}

\section{Design of a Lexical-Analyzer Generator}
\subsection{DFA's for Lexical Analyzers}

We simulate the DFA until at some point there is no next state (or strictly speaking, the next state is $\varnothing$, the \textit{dead state} corresponding to the empty set of NFA states).

% %--------------------------------------------------------------------------
% %         Bibliographie 
% %--------------------------------------------------------------------------
\end{document}